from string import whitespace
from typing import (
    Container,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
)

from errors import (
    IllegalCharError,
    UnexpectedEOFError,
    UnexpectedTokenError,
)
from log import logger
from .tokens import (
    COMMENT_MARKER,
    DOUBLE_CHAR_TOKENS,
    KEYWORDS,
    SINGLE_CHAR_TOKENS,
    TokenTypes,
)

Token = NamedTuple(
    "Token",
    (("span", Tuple[int, int]), ("type_", TokenTypes), ("value", Optional[str])),
)


DOUBLE_CHAR_VALUES: Container[str] = [type_.value for type_ in DOUBLE_CHAR_TOKENS]
KEYWORD_VALUES: Container[str] = [type_.value for type_ in KEYWORDS]
SINGLE_CHAR_VALUES: Container[str] = [type_.value for type_ in SINGLE_CHAR_TOKENS]


def lex(
    source: str, ignore: Container[TokenTypes] = (TokenTypes.comment,)
) -> "TokenStream":
    """
    Generate a stream of tokens for the parser to build an AST with.

    Parameters
    ----------
    source: str
        The string that will be lexed.
    ignore: Container[TokenTypes]
        The token types that shouldn't be exposed to the client.

    Returns
    -------
    TokenStream
        The tokens that were generated by lexing.
    """
    stream = []
    prev_end = 0
    source_length = len(source)
    while prev_end < source_length:
        result = lex_word(source[prev_end:])
        if result is None:
            raise IllegalCharError((prev_end, prev_end + 1), source[prev_end])

        token_type, value, length = result
        start, prev_end = prev_end, prev_end + length
        stream.append(Token((start, prev_end), token_type, value))

    return TokenStream(stream, ignore)


def lex_word(source: str) -> Optional[Tuple[TokenTypes, Optional[str], int]]:
    """Create the data required to build a single lexeme."""
    first = source[0]
    if first.isdecimal():
        return lex_number(source)
    if first.isalnum() or first == "_":
        return lex_name(source)
    if first == '"':
        return lex_string(source)
    if source[:2] in DOUBLE_CHAR_VALUES:
        return TokenTypes(source[:2]), None, 2
    if first in SINGLE_CHAR_VALUES:
        return TokenTypes(first), None, 1
    if first == COMMENT_MARKER:
        return lex_comment(source)
    if first in whitespace:
        return lex_whitespace(source)
    return None


def lex_comment(source: str) -> Tuple[TokenTypes, Optional[str], int]:
    """Lex a single line comment."""
    max_index = len(source)
    current_index = 0
    while current_index < max_index and source[current_index] != "\n":
        current_index += 1

    current_index += 1 if current_index < max_index else 0
    return TokenTypes.comment, source[:current_index], current_index


def lex_name(source: str) -> Tuple[TokenTypes, Optional[str], int]:
    """
    Parse the (truncated) source in order to create either a `name`
    or a keyword token.

    Parameters
    ---------
    source: str
        The source code that will be lexed.

    Returns
    -------
    Tuple[TokenTypes, Optional[str], int]
        It is a tuple of either a keyword token type or
        `TokenTypes.name`, then the actual name parsed (or `None` if
        it's a keyword) and its length.
    """
    max_index = len(source)
    current_index = 0
    current_char = source[current_index]
    while current_char.isalnum() or current_char == "_":
        current_index += 1
        if current_index >= max_index:
            break
        current_char = source[current_index]

    token_value = source[:current_index]
    if token_value in KEYWORD_VALUES:
        return TokenTypes(token_value), None, current_index
    if token_value[0].isupper():
        return TokenTypes.type_name, token_value, current_index
    return TokenTypes.name, token_value, current_index


def lex_string(source: str) -> Optional[Tuple[TokenTypes, Optional[str], int]]:
    """
    Parse the (truncated) source in order to create a string token.

    Parameters
    ---------
    source: str
        The source code that will be lexed.

    Returns
    -------
    Optional[Tuple[TokenTypes, str, int]]
        If it is `None`, then it was unable to parse the source. Else,
        it is a tuple of (specifically) `TokenTypes.string`, then
        the actual string parsed and its length.
    """
    current_index = 1
    in_escape = False
    max_index = len(source)
    while current_index < max_index:
        if (not in_escape) and source[current_index] == '"':
            break

        in_escape = (not in_escape) if source[current_index] == "\\" else False
        current_index += 1
    else:
        logger.critical(
            "The stream unexpectedly ended before finding the end of the string."
        )
        return None

    current_index += 1
    return TokenTypes.string, source[:current_index], current_index


def lex_number(source: str) -> Tuple[TokenTypes, Optional[str], int]:
    """
    Parse the (truncated) source in order to create either an `integer`
    or a `float_` token.

    Parameters
    ---------
    source: str
        The source code that will be lexed.

    Returns
    -------
    Tuple[TokenTypes, str, int]
        It is a tuple of (specifically) either `TokenTypes.integer` or
        `TokenTypes.float_`, then the actual string parsed and its
        length.
    """
    max_index = len(source)
    current_index = 0
    type_ = TokenTypes.integer
    while current_index < max_index and source[current_index].isdecimal():
        current_index += 1

    if current_index < max_index and source[current_index] == ".":
        current_index += 1
        type_ = TokenTypes.float_
        while current_index < max_index and source[current_index].isdecimal():
            current_index += 1

    return type_, source[:current_index], current_index


def lex_whitespace(source: str) -> Tuple[TokenTypes, Optional[str], int]:
    """Lex either a `whitespace` or a `newline` token."""
    max_index = len(source)
    current_index = 0
    while current_index < max_index and source[current_index] in whitespace:
        current_index += 1
    return TokenTypes.whitespace, source[:current_index], current_index


class TokenStream:
    """
    A wrapper class around the token generator so that we can preserve
    already computed elements and integrate with the parser which
    expects an eager lexer.

    Warnings
    --------
    - This class contains a lot of mutable state so the best way to use
      it is by having a separate copy for each thread.
    """

    __slots__ = ("_index", "_tokens", "ignore")

    def __init__(self, tokens: Sequence[Token], ignore: Container[TokenTypes]) -> None:
        self._index: int = 0
        self._tokens: Sequence[Token] = tokens
        self.ignore: Container[TokenTypes] = ignore

    def consume(self, *expected: TokenTypes) -> Token:
        """
        Check if the next token is in `expected` and if it is, return
        the token at the head and next the stream. If it's not in
        the stream, raise an error.

        Returns
        -------
        Token
            The token at the head of the stream.
        """
        head = self.next()
        if head.type_ in expected:
            return head
        logger.critical("Tried consuming %s but got %s", expected, head)
        raise UnexpectedTokenError(head, *expected)

    def consume_if(self, *expected: TokenTypes) -> bool:
        """
        Check if the next token is in `expected` and if it is, next
        one step through the stream. Otherwise, keep the stream as is.

        Parameters
        ----------
        *expected: TokenTypes
            It is expected that the `type_` attr of tokens at the head
            of `stream` should be one of these.

        Raises
        ------
        error.StreamOverError
            There is nothing left in the `stream` so we can't next
            it.

        Returns
        -------
        bool
            Whether `expected` was found at the front of the stream.
        """
        if self.peek(*expected):
            self.consume(*expected)
            return True
        return False

    def next(self) -> Token:
        """
        Move the stream forward one step.

        Raises
        ------
        error.StreamOverError
            There is nothing left in `stream` so we can't advance it.

        Returns
        -------
        Token
            The token at the head of the stream.
        """
        try:
            token = self._tokens[self._index]
        except IndexError as error:
            raise UnexpectedEOFError() from error
        else:
            self._index += 1
            return self.next() if token.type_ in self.ignore else token

    def peek(self, *expected: TokenTypes) -> bool:
        """
        Check if `expected` is the next token without advancing the
        stream.

        Warnings
        --------
        - If the stream is empty, then `False` will be returned.

        Parameters
        ----------
        *expected: TokenTypes
            It is expected that the `type_` attr of tokens at the head
            of `stream` should be one of these.

        Returns
        -------
        bool
            Whether `expected` was found at the front of the stream.
        """
        token = self.preview()
        return token is not None and token.type_ in expected

    def preview(self) -> Optional[Token]:
        """
        View the token at the head of the stream without letting the
        stream forget about it or return `None` if the stream is empty.

        Returns
        -------
        Token
            The token at the head of the stream.
        """
        try:
            head = self.next()
            self._index -= 1
            return head
        except UnexpectedEOFError:
            return None

    def show(self, sep: str = "\n") -> str:
        """Pretty print the tokens contained."""
        parts = []
        for token in self:
            span = f"{token.span[0]}-{token.span[1]}"
            parts.append(
                f"[ #{span} {token.type_.name} ]"
                if token.value is None
                else f'[ #{span} {token.type_.name} "{token.value}" ]'
            )

        return sep.join(parts)

    def __bool__(self):
        return self.preview() is not None

    def __iter__(self):
        while self:
            yield self.next()

    def __next__(self):
        try:
            return self.next()
        except UnexpectedEOFError as error:
            raise StopIteration() from error

    def __str__(self):
        return f"[ {self.show(', ')} ]"

    def __repr__(self):
        parts = []
        for index, token in enumerate(self._tokens):
            if token.type_ not in self.ignore:
                span = f"{token.span[0]}-{token.span[1]}"
                parts.append(
                    f"[ #{span} {token.type_.name} ]"
                    if token.value is None
                    else f'[ #{span} {token.type_.name} "{token.value}" ]'
                )

            if index == self._index:
                parts[-1] = f">{parts[-1]}<"
        return f"[ {' , '.join(parts)} ]"
